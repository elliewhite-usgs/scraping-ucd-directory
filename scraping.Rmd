---
title: "Scraping the UC Davis Directory"
author: "Ellie White"
date: "February 1, 2018"
output: html_document
---

```{r data}
# first read in the sample names you want to scrape the UC Davis directroy for
names <- read.csv("inputdata/samplenames.csv", header=FALSE, stringsAsFactors=FALSE)

library(RCurl)
# for some reason this one doesn't work
# u <- "https://directory.ucdavis.edu/directory/PeopleSearch.htm"

# use this one!!! go to the people search tab and find the url
u <- "https://directory.ucdavis.edu/search/directory_results.shtml?"

# go to the url above in firefox and open up webdeveloper -> Toggle tools
# in Network, all look for status 200 (meaning it was successful). If Method is "Get" use getForms is Method is "Post" use postForms. 
# in the Params, look for your input. Name value pairs of this will be sent to the function below
# con <-  getCurlHandle(followlocation = TRUE, cookiejar = "", verbose = TRUE, useragent = "R")
# tt <-  getURLContent(u, curl = con)

txt <- getForm(u, filter= "Elaheh White") 

library(XML)
doc <- htmlParse(txt, asText = TRUE)
grep("Elaheh White", txt) # to see if it got anything from the search

# go to inspect element in the webpage and find the pattern to specify in the getNodeSet function below
node <- getNodeSet(doc,"//div[@id='directory_results_wrapper']//table/tr/td")
sapply(node, xmlValue, simplify=TRUE)
xmlSApply(node, xmlValue)

# define the get directory function from what you learned above
getdir <- function(n){
  txt <- getForm(u, filter= n)
  doc <- htmlParse(txt, asText = TRUE)
  node <- getNodeSet(doc,"//div[@id='directory_results_wrapper']//table/tr/td")
  sapply(node, xmlValue, simplify = TRUE)
}

# apply the function above to each member of the list of names
info <- lapply(names$V1, FUN=getdir)

# put it in a dataframe so you can write it out, first initialize an empty data frame
dat <- data.frame(matrix(NA, nrow=length(info), ncol=lapply(info, length)[[which.max(lapply(info, length))]]))

# remove the empty lists
info_collected <- info[which(lapply(info, length)!=0)]

for(i in seq_along(info_collected)){
  for(j in 1:(lapply(info_collected, length)[[which.max(lapply(info_collected, length))]])){
    dat[i,j] <- info_collected[[i]][j]
  }
} 

write.csv(dat, "outputdata/fullinfo.csv")

# # now clean it up and add to the csv, I'm not doing it this way anymore
# listy <- sapply(info, FUN=function(x){
#   strsplit(x, split="Name|Update|E-mail:|Card:|Title|Department|Level|Major")
#   })
# listy
```

# Citations
```{r citations}
# cite R 
toBibtex(citation())

# cite packages
citethese <- c("RCurl", "XML")

for(i in seq_along(citethese)){
  x <- citation(citethese[i])
  print(x)
  # print(toBibtex(x))
}

remove(x)
remove(i)
remove(citethese)
```
